#!/usr/bin/env python3
"""
ReadingTurtle ë‹¨ì–´ ë°ì´í„° í¬ë¡¤ë§ ìŠ¤í¬ë¦½íŠ¸
- ê° ISBNë³„ ë‹¨ì–´ ëª©ë¡ì„ ê°€ì ¸ì™€ì„œ word_lists í…Œì´ë¸”ì— ì €ì¥
- ê° ë‹¨ì–´ì˜ ì •ì˜ë¥¼ ê°€ì ¸ì™€ì„œ word_definitions í…Œì´ë¸”ì— ì €ì¥
- ë¹„ë™ê¸° ì²˜ë¦¬ë¡œ 200ê°œì”© ë™ì‹œ ì²˜ë¦¬
"""

import asyncio
import aiohttp
import psycopg2
from psycopg2.extras import execute_values
import json
from datetime import datetime
from typing import List, Dict, Optional
import sys

# ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì •ë³´
DB_CONFIG = {
    'dbname': 'booktaco',
    'user': 'booktaco_user',
    'password': 'ares82',
    'host': 'localhost',
    'port': 5432
}

# API ì„¤ì •
API_BASE_URL = 'https://app.booktaco.com'
PHPSESSID = '2cf84d80dc03ed6e2aad72525f038e6a'
BATCH_SIZE = 200  # ë™ì‹œ ì²˜ë¦¬í•  ISBN ìˆ˜
WORD_BATCH_SIZE = 50  # ë™ì‹œ ì²˜ë¦¬í•  ë‹¨ì–´ ìˆ˜

# í†µê³„
stats = {
    'total_books': 0,
    'books_processed': 0,
    'books_with_words': 0,
    'total_words_in_lists': 0,
    'unique_words_saved': 0,
    'words_already_existed': 0,
    'errors': 0
}


def get_db_connection():
    """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ìƒì„±"""
    return psycopg2.connect(**DB_CONFIG)


def get_all_isbns() -> List[str]:
    """books í…Œì´ë¸”ì—ì„œ ëª¨ë“  ISBN ê°€ì ¸ì˜¤ê¸°"""
    conn = get_db_connection()
    cursor = conn.cursor()

    cursor.execute("SELECT isbn FROM books ORDER BY isbn")
    isbns = [row[0] for row in cursor.fetchall()]

    cursor.close()
    conn.close()

    return isbns


async def fetch_word_list(session: aiohttp.ClientSession, isbn: str) -> Optional[List[str]]:
    """íŠ¹ì • ISBNì˜ ë‹¨ì–´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    url = f"{API_BASE_URL}/teacher/manage-books/list-wordlist"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Content-Type': 'application/json',
        'Cookie': f'PHPSESSID={PHPSESSID}'
    }
    payload = json.dumps({"isbn": isbn})

    try:
        async with session.post(url, headers=headers, data=payload, timeout=30) as response:
            if response.status == 200:
                data = await response.json()
                if data.get('status') == 'success':
                    return data.get('data', [])
            return None
    except Exception as e:
        print(f"âŒ Error fetching word list for ISBN {isbn}: {e}")
        stats['errors'] += 1
        return None


async def fetch_word_definition(session: aiohttp.ClientSession, word: str, debug: bool = False) -> Optional[Dict]:
    """íŠ¹ì • ë‹¨ì–´ì˜ ì •ì˜ ê°€ì ¸ì˜¤ê¸°"""
    url = f"{API_BASE_URL}/api/get-word?action=word-definition"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Content-Type': 'application/json',
        'Cookie': f'PHPSESSID={PHPSESSID}'
    }
    payload = json.dumps({"word": word})

    try:
        async with session.post(url, headers=headers, data=payload, timeout=30) as response:
            if debug:
                print(f"\nğŸ” DEBUG - Word: '{word}'")
                print(f"   Status: {response.status}")
                print(f"   Content-Type: {response.headers.get('Content-Type')}")

            if response.status == 200:
                response_text = await response.text()

                if debug:
                    print(f"   Response length: {len(response_text)} characters")
                    print(f"   First 500 chars: {response_text[:500]}")

                try:
                    data = json.loads(response_text)
                    if debug:
                        print(f"   JSON parsed successfully")
                        print(f"   Keys: {list(data.keys())}")
                        print(f"   Full response: {json.dumps(data, indent=2, ensure_ascii=False)}")

                    if data.get('request') == 'success':
                        word_data = data.get('data', {})
                        return {
                            'word_id': word_data.get('WordID'),
                            'word': word_data.get('word'),
                            'definition': word_data.get('definition'),
                            'example_sentence': word_data.get('sentence')
                        }
                except json.JSONDecodeError as je:
                    if debug:
                        print(f"   âŒ JSON decode error: {je}")
                    raise
            return None
    except Exception as e:
        print(f"âŒ Error fetching definition for word '{word}': {e}")
        stats['errors'] += 1
        return None


def word_definition_exists(cursor, word: str) -> bool:
    """ë‹¨ì–´ ì •ì˜ê°€ ì´ë¯¸ DBì— ìˆëŠ”ì§€ í™•ì¸"""
    cursor.execute("SELECT 1 FROM word_definitions WHERE word = %s", (word,))
    return cursor.fetchone() is not None


def save_word_definition(cursor, word_data: Dict):
    """ë‹¨ì–´ ì •ì˜ë¥¼ DBì— ì €ì¥"""
    try:
        cursor.execute("""
            INSERT INTO word_definitions (word_id, word, definition, example_sentence)
            VALUES (%s, %s, %s, %s)
            ON CONFLICT (word_id) DO UPDATE SET
                definition = EXCLUDED.definition,
                example_sentence = EXCLUDED.example_sentence,
                updated_at = CURRENT_TIMESTAMP
        """, (
            word_data['word_id'],
            word_data['word'],
            word_data['definition'],
            word_data['example_sentence']
        ))
        return True
    except Exception as e:
        print(f"âŒ Error saving word definition for '{word_data['word']}': {e}")
        stats['errors'] += 1
        return False


def save_word_list(cursor, isbn: str, words: List[str]):
    """ì±…ì˜ ë‹¨ì–´ ëª©ë¡ì„ DBì— ì €ì¥"""
    try:
        # ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•´)
        cursor.execute("DELETE FROM word_lists WHERE isbn = %s", (isbn,))

        # ìƒˆ ë°ì´í„° ì‚½ì…
        data = [(isbn, word, idx + 1) for idx, word in enumerate(words)]
        execute_values(cursor, """
            INSERT INTO word_lists (isbn, word, word_order)
            VALUES %s
            ON CONFLICT (isbn, word) DO NOTHING
        """, data)

        return True
    except Exception as e:
        print(f"âŒ Error saving word list for ISBN {isbn}: {e}")
        stats['errors'] += 1
        return False


async def process_words_batch(session: aiohttp.ClientSession, words: List[str], cursor, debug: bool = False) -> int:
    """ë‹¨ì–´ ë°°ì¹˜ ì²˜ë¦¬ - ì •ì˜ ê°€ì ¸ì˜¤ê¸°"""
    tasks = []
    words_to_fetch = []

    # DBì— ì—†ëŠ” ë‹¨ì–´ë§Œ í•„í„°ë§
    for word in words:
        if not word_definition_exists(cursor, word):
            words_to_fetch.append(word)
            # ë””ë²„ê·¸ ëª¨ë“œì¼ ë•ŒëŠ” ì²« 3ê°œë§Œ ë””ë²„ê·¸
            is_debug = debug and len(tasks) < 3
            tasks.append(fetch_word_definition(session, word, debug=is_debug))
        else:
            stats['words_already_existed'] += 1

    if not tasks:
        return 0

    # ë¹„ë™ê¸°ë¡œ ë‹¨ì–´ ì •ì˜ ê°€ì ¸ì˜¤ê¸°
    results = await asyncio.gather(*tasks)

    # DBì— ì €ì¥
    saved_count = 0
    for word_data in results:
        if word_data and word_data['word_id']:
            if save_word_definition(cursor, word_data):
                saved_count += 1
                stats['unique_words_saved'] += 1

    return saved_count


async def process_isbn(session: aiohttp.ClientSession, isbn: str, cursor, debug: bool = False) -> bool:
    """ë‹¨ì¼ ISBN ì²˜ë¦¬"""
    # 1. ë‹¨ì–´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    word_list = await fetch_word_list(session, isbn)

    if not word_list or len(word_list) == 0:
        print(f"âš ï¸  ISBN {isbn}: No words found")
        return False

    print(f"ğŸ“š ISBN {isbn}: {len(word_list)} words found")
    if debug:
        print(f"   Sample words: {word_list[:10]}")

    stats['books_with_words'] += 1
    stats['total_words_in_lists'] += len(word_list)

    # 2. ë‹¨ì–´ ëª©ë¡ ì €ì¥
    save_word_list(cursor, isbn, word_list)
    if debug:
        print(f"   âœ… Word list saved to DB")

    # 3. ë‹¨ì–´ ì •ì˜ ê°€ì ¸ì˜¤ê¸° (ë°°ì¹˜ ì²˜ë¦¬)
    unique_words = list(set(word_list))  # ì¤‘ë³µ ì œê±°
    if debug:
        print(f"   Unique words: {len(unique_words)}")

    # ë‹¨ì–´ë¥¼ ì‘ì€ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    for i in range(0, len(unique_words), WORD_BATCH_SIZE):
        batch = unique_words[i:i + WORD_BATCH_SIZE]
        if debug:
            print(f"   Processing word batch {i//WORD_BATCH_SIZE + 1}/{(len(unique_words)-1)//WORD_BATCH_SIZE + 1}")
        await process_words_batch(session, batch, cursor, debug=debug)
        await asyncio.sleep(0.1)  # API ë¶€í•˜ ë°©ì§€

    return True


async def process_isbn_batch(isbns: List[str], batch_num: int, total_batches: int):
    """ISBN ë°°ì¹˜ ì²˜ë¦¬"""
    print(f"\n{'='*60}")
    print(f"ğŸ“¦ Batch {batch_num}/{total_batches} - Processing {len(isbns)} ISBNs")
    print(f"{'='*60}")

    conn = get_db_connection()
    cursor = conn.cursor()

    async with aiohttp.ClientSession() as session:
        tasks = [process_isbn(session, isbn, cursor) for isbn in isbns]
        results = await asyncio.gather(*tasks)

        stats['books_processed'] += len(isbns)

        # ê° ISBN ì²˜ë¦¬ í›„ ì»¤ë°‹
        conn.commit()

    cursor.close()
    conn.close()

    # ì§„í–‰ ìƒí™© ì¶œë ¥
    print(f"\nğŸ“Š Batch {batch_num} Complete:")
    print(f"   - Books processed: {stats['books_processed']}/{stats['total_books']}")
    print(f"   - Books with words: {stats['books_with_words']}")
    print(f"   - Total words in lists: {stats['total_words_in_lists']}")
    print(f"   - Unique words saved: {stats['unique_words_saved']}")
    print(f"   - Words already existed: {stats['words_already_existed']}")
    print(f"   - Errors: {stats['errors']}")


async def test_single_isbn():
    """í…ŒìŠ¤íŠ¸: ë‹¨ì¼ ISBN ì²˜ë¦¬"""
    test_isbn = "9780399541582"

    print("="*60)
    print("ğŸ§ª TEST MODE - Single ISBN")
    print("="*60)
    print(f"ğŸ“… Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ğŸ“– Test ISBN: {test_isbn}")
    print(f"âš™ï¸  Word batch size: {WORD_BATCH_SIZE} words per batch")
    print()

    conn = get_db_connection()
    cursor = conn.cursor()

    async with aiohttp.ClientSession() as session:
        print("Starting test with detailed debugging...\n")
        await process_isbn(session, test_isbn, cursor, debug=True)
        conn.commit()

    cursor.close()
    conn.close()

    print("\n" + "="*60)
    print("ğŸ§ª TEST COMPLETED")
    print("="*60)
    print("ğŸ“Š Test Statistics:")
    print(f"   - Books with words: {stats['books_with_words']}")
    print(f"   - Total words in lists: {stats['total_words_in_lists']}")
    print(f"   - Unique words saved: {stats['unique_words_saved']}")
    print(f"   - Words already existed: {stats['words_already_existed']}")
    print(f"   - Errors: {stats['errors']}")
    print("="*60)


async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("="*60)
    print("ğŸ¢ ReadingTurtle Word Crawler Started")
    print("="*60)
    print(f"ğŸ“… Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"âš™ï¸  Batch size: {BATCH_SIZE} ISBNs per batch")
    print(f"âš™ï¸  Word batch size: {WORD_BATCH_SIZE} words per batch")
    print()

    # 1. ëª¨ë“  ISBN ê°€ì ¸ì˜¤ê¸°
    print("ğŸ“– Fetching all ISBNs from database...")
    isbns = get_all_isbns()
    stats['total_books'] = len(isbns)
    print(f"âœ… Found {len(isbns)} books")

    if len(isbns) == 0:
        print("âŒ No books found in database!")
        return

    # 2. ISBNì„ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    batches = [isbns[i:i + BATCH_SIZE] for i in range(0, len(isbns), BATCH_SIZE)]
    total_batches = len(batches)

    print(f"ğŸ“¦ Split into {total_batches} batches\n")

    # 3. ê° ë°°ì¹˜ ì²˜ë¦¬
    start_time = datetime.now()

    for batch_num, batch in enumerate(batches, 1):
        try:
            await process_isbn_batch(batch, batch_num, total_batches)
        except Exception as e:
            print(f"âŒ Error processing batch {batch_num}: {e}")
            stats['errors'] += 1
            continue

    # 4. ìµœì¢… í†µê³„
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print("\n" + "="*60)
    print("âœ… CRAWLING COMPLETED")
    print("="*60)
    print(f"ğŸ“… End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"â±ï¸  Duration: {duration:.2f} seconds ({duration/60:.2f} minutes)")
    print()
    print("ğŸ“Š Final Statistics:")
    print(f"   - Total books: {stats['total_books']}")
    print(f"   - Books processed: {stats['books_processed']}")
    print(f"   - Books with words: {stats['books_with_words']}")
    print(f"   - Total words in lists: {stats['total_words_in_lists']}")
    print(f"   - Unique words saved: {stats['unique_words_saved']}")
    print(f"   - Words already existed: {stats['words_already_existed']}")
    print(f"   - Total unique words: {stats['unique_words_saved'] + stats['words_already_existed']}")
    print(f"   - Errors: {stats['errors']}")
    print()
    print(f"âš¡ Average speed: {stats['books_processed']/duration:.2f} books/second")
    print("="*60)


if __name__ == "__main__":
    try:
        # í…ŒìŠ¤íŠ¸ ëª¨ë“œ ì‹¤í–‰
        print("ğŸ§ª Running in TEST mode first...")
        asyncio.run(test_single_isbn())

        print("\n\n" + "="*60)
        print("â¸ï¸  TEST COMPLETE - Ready for full crawl")
        print("="*60)
        print("Press ENTER to continue with full crawl, or Ctrl+C to exit...")
        input()

        # í†µê³„ ë¦¬ì…‹
        stats['books_processed'] = 0
        stats['books_with_words'] = 0
        stats['total_words_in_lists'] = 0
        stats['unique_words_saved'] = 0
        stats['words_already_existed'] = 0
        stats['errors'] = 0

        # ì „ì²´ í¬ë¡¤ë§ ì‹¤í–‰
        asyncio.run(main())

    except KeyboardInterrupt:
        print("\n\nâš ï¸  Interrupted by user")
        print(f"ğŸ“Š Partial Statistics:")
        print(f"   - Books processed: {stats['books_processed']}/{stats['total_books']}")
        print(f"   - Books with words: {stats['books_with_words']}")
        print(f"   - Unique words saved: {stats['unique_words_saved']}")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
