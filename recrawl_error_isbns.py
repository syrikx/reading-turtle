#!/usr/bin/env python3
"""
ReadingTurtle - Error ISBN Ïû¨ÌÅ¨Î°§ÎßÅ Ïä§ÌÅ¨Î¶ΩÌä∏
- isbn_with_error_message_only.txt ÌååÏùºÏùò ISBNÎì§Ïóê ÎåÄÌï¥ Îã®Ïñ¥ Î™©Î°ùÏùÑ Îã§Ïãú ÌÅ¨Î°§ÎßÅ
- Í∏∞Ï°¥Ïùò 'error', 'message' Îã®Ïñ¥Î•º ÏÇ≠Ï†úÌïòÍ≥† ÏÉàÎ°úÏö¥ Îã®Ïñ¥ Î™©Î°ùÏúºÎ°ú ÍµêÏ≤¥
"""

import asyncio
import aiohttp
import psycopg2
from psycopg2.extras import execute_values
import json
from datetime import datetime
from typing import List, Dict, Optional
import sys

# Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ Ï†ïÎ≥¥
DB_CONFIG = {
    'dbname': 'booktaco',
    'user': 'booktaco_user',
    'password': 'ares82',
    'host': 'localhost',
    'port': 5432
}

# API ÏÑ§Ï†ï
API_BASE_URL = 'https://app.booktaco.com'
PHPSESSID = '2cf84d80dc03ed6e2aad72525f038e6a'
BATCH_SIZE = 200  # ÎèôÏãú Ï≤òÎ¶¨Ìï† ISBN Ïàò
WORD_BATCH_SIZE = 50  # ÎèôÏãú Ï≤òÎ¶¨Ìï† Îã®Ïñ¥ Ïàò

# ÌÜµÍ≥Ñ
stats = {
    'total_books': 0,
    'books_processed': 0,
    'books_with_words': 0,
    'books_still_error': 0,
    'total_words_in_lists': 0,
    'unique_words_saved': 0,
    'words_already_existed': 0,
    'errors': 0
}


def get_db_connection():
    """Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ïó∞Í≤∞ ÏÉùÏÑ±"""
    return psycopg2.connect(**DB_CONFIG)


def read_isbn_list(filename: str) -> List[str]:
    """ÌååÏùºÏóêÏÑú ISBN Î™©Î°ù ÏùΩÍ∏∞"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            isbns = [line.strip() for line in f if line.strip()]
        return isbns
    except Exception as e:
        print(f"‚ùå Error reading file {filename}: {e}")
        return []


async def fetch_word_list(session: aiohttp.ClientSession, isbn: str) -> Optional[List[str]]:
    """ÌäπÏ†ï ISBNÏùò Îã®Ïñ¥ Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞"""
    url = f"{API_BASE_URL}/teacher/manage-books/list-wordlist"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Content-Type': 'application/json',
        'Cookie': f'PHPSESSID={PHPSESSID}'
    }
    payload = json.dumps({"isbn": isbn})

    try:
        async with session.post(url, headers=headers, data=payload, timeout=30) as response:
            if response.status == 200:
                data = await response.json()
                if data.get('status') == 'success':
                    return data.get('data', [])
            return None
    except Exception as e:
        print(f"‚ùå Error fetching word list for ISBN {isbn}: {e}")
        stats['errors'] += 1
        return None


async def fetch_word_definition(session: aiohttp.ClientSession, word: str) -> Optional[Dict]:
    """ÌäπÏ†ï Îã®Ïñ¥Ïùò Ï†ïÏùò Í∞ÄÏ†∏Ïò§Í∏∞"""
    url = f"{API_BASE_URL}/api/get-word?action=word-definition"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Content-Type': 'application/json',
        'Cookie': f'PHPSESSID={PHPSESSID}'
    }
    payload = json.dumps({"word": word})

    try:
        async with session.post(url, headers=headers, data=payload, timeout=30) as response:
            if response.status == 200:
                response_text = await response.text()
                try:
                    data = json.loads(response_text)
                    if data.get('request') == 'success':
                        word_data = data.get('data', {})
                        return {
                            'word_id': word_data.get('WordID'),
                            'word': word_data.get('word'),
                            'definition': word_data.get('definition'),
                            'example_sentence': word_data.get('sentence')
                        }
                except json.JSONDecodeError:
                    pass
            return None
    except Exception as e:
        stats['errors'] += 1
        return None


def word_definition_exists(cursor, word: str) -> bool:
    """Îã®Ïñ¥ Ï†ïÏùòÍ∞Ä Ïù¥ÎØ∏ DBÏóê ÏûàÎäîÏßÄ ÌôïÏù∏"""
    cursor.execute("SELECT 1 FROM word_definitions WHERE word = %s", (word,))
    return cursor.fetchone() is not None


def save_word_definition(cursor, word_data: Dict):
    """Îã®Ïñ¥ Ï†ïÏùòÎ•º DBÏóê Ï†ÄÏû•"""
    try:
        cursor.execute("""
            INSERT INTO word_definitions (word_id, word, definition, example_sentence)
            VALUES (%s, %s, %s, %s)
            ON CONFLICT (word_id) DO UPDATE SET
                definition = EXCLUDED.definition,
                example_sentence = EXCLUDED.example_sentence,
                updated_at = CURRENT_TIMESTAMP
        """, (
            word_data['word_id'],
            word_data['word'],
            word_data['definition'],
            word_data['example_sentence']
        ))
        return True
    except Exception as e:
        print(f"‚ùå Error saving word definition for '{word_data['word']}': {e}")
        stats['errors'] += 1
        return False


def save_word_list(cursor, isbn: str, words: List[str]):
    """Ï±ÖÏùò Îã®Ïñ¥ Î™©Î°ùÏùÑ DBÏóê Ï†ÄÏû• (Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú ÌõÑ ÏÉàÎ°ú ÏÇΩÏûÖ)"""
    try:
        # Í∏∞Ï°¥ Îç∞Ïù¥ÌÑ∞ ÏÇ≠Ï†ú
        cursor.execute("DELETE FROM word_lists WHERE isbn = %s", (isbn,))

        # ÏÉà Îç∞Ïù¥ÌÑ∞ ÏÇΩÏûÖ
        if words:
            data = [(isbn, word, idx + 1) for idx, word in enumerate(words)]
            execute_values(cursor, """
                INSERT INTO word_lists (isbn, word, word_order)
                VALUES %s
                ON CONFLICT (isbn, word) DO NOTHING
            """, data)

        return True
    except Exception as e:
        print(f"‚ùå Error saving word list for ISBN {isbn}: {e}")
        stats['errors'] += 1
        return False


async def process_words_batch(session: aiohttp.ClientSession, words: List[str], cursor) -> int:
    """Îã®Ïñ¥ Î∞∞Ïπò Ï≤òÎ¶¨ - Ï†ïÏùò Í∞ÄÏ†∏Ïò§Í∏∞"""
    tasks = []
    words_to_fetch = []

    # DBÏóê ÏóÜÎäî Îã®Ïñ¥Îßå ÌïÑÌÑ∞ÎßÅ
    for word in words:
        if not word_definition_exists(cursor, word):
            words_to_fetch.append(word)
            tasks.append(fetch_word_definition(session, word))
        else:
            stats['words_already_existed'] += 1

    if not tasks:
        return 0

    # ÎπÑÎèôÍ∏∞Î°ú Îã®Ïñ¥ Ï†ïÏùò Í∞ÄÏ†∏Ïò§Í∏∞
    results = await asyncio.gather(*tasks)

    # DBÏóê Ï†ÄÏû•
    saved_count = 0
    for word_data in results:
        if word_data and word_data.get('word_id'):
            if save_word_definition(cursor, word_data):
                saved_count += 1
                stats['unique_words_saved'] += 1

    return saved_count


async def process_isbn(session: aiohttp.ClientSession, isbn: str, cursor) -> bool:
    """Îã®Ïùº ISBN Ï≤òÎ¶¨"""
    # 1. Îã®Ïñ¥ Î™©Î°ù Í∞ÄÏ†∏Ïò§Í∏∞
    word_list = await fetch_word_list(session, isbn)

    if not word_list or len(word_list) == 0:
        print(f"‚ö†Ô∏è  ISBN {isbn}: No words found (still error)")
        stats['books_still_error'] += 1
        # Îã®Ïñ¥ Î™©Î°ùÏù¥ ÏóÜÏñ¥ÎèÑ Í∏∞Ï°¥Ïùò error/messageÎäî ÏÇ≠Ï†ú
        save_word_list(cursor, isbn, [])
        return False

    # error, messageÎßå ÏûàÎäî Í≤ΩÏö∞ÎèÑ Ï≤¥ÌÅ¨
    if len(word_list) == 2 and set(word_list) == {'error', 'message'}:
        print(f"‚ö†Ô∏è  ISBN {isbn}: Still has error/message only")
        stats['books_still_error'] += 1
        save_word_list(cursor, isbn, [])
        return False

    print(f"‚úÖ ISBN {isbn}: {len(word_list)} words found")
    stats['books_with_words'] += 1
    stats['total_words_in_lists'] += len(word_list)

    # 2. Îã®Ïñ¥ Î™©Î°ù Ï†ÄÏû• (Í∏∞Ï°¥ error/message ÏÇ≠Ï†úÎê®)
    save_word_list(cursor, isbn, word_list)

    # 3. Îã®Ïñ¥ Ï†ïÏùò Í∞ÄÏ†∏Ïò§Í∏∞ (Î∞∞Ïπò Ï≤òÎ¶¨)
    unique_words = list(set(word_list))  # Ï§ëÎ≥µ Ï†úÍ±∞

    # Îã®Ïñ¥Î•º ÏûëÏùÄ Î∞∞ÏπòÎ°ú ÎÇòÎàÑÏñ¥ Ï≤òÎ¶¨
    for i in range(0, len(unique_words), WORD_BATCH_SIZE):
        batch = unique_words[i:i + WORD_BATCH_SIZE]
        await process_words_batch(session, batch, cursor)
        await asyncio.sleep(0.1)  # API Î∂ÄÌïò Î∞©ÏßÄ

    return True


async def process_isbn_batch(isbns: List[str], batch_num: int, total_batches: int):
    """ISBN Î∞∞Ïπò Ï≤òÎ¶¨"""
    print(f"\n{'='*60}")
    print(f"üì¶ Batch {batch_num}/{total_batches} - Processing {len(isbns)} ISBNs")
    print(f"{'='*60}")

    conn = get_db_connection()
    cursor = conn.cursor()

    async with aiohttp.ClientSession() as session:
        tasks = [process_isbn(session, isbn, cursor) for isbn in isbns]
        results = await asyncio.gather(*tasks)

        stats['books_processed'] += len(isbns)

        # Í∞Å ISBN Ï≤òÎ¶¨ ÌõÑ Ïª§Î∞ã
        conn.commit()

    cursor.close()
    conn.close()

    # ÏßÑÌñâ ÏÉÅÌô© Ï∂úÎ†•
    print(f"\nüìä Batch {batch_num} Complete:")
    print(f"   - Books processed: {stats['books_processed']}/{stats['total_books']}")
    print(f"   - Books with words: {stats['books_with_words']}")
    print(f"   - Books still error: {stats['books_still_error']}")
    print(f"   - Total words in lists: {stats['total_words_in_lists']}")
    print(f"   - Unique words saved: {stats['unique_words_saved']}")
    print(f"   - Words already existed: {stats['words_already_existed']}")
    print(f"   - Errors: {stats['errors']}")


async def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    print("="*60)
    print("üê¢ ReadingTurtle - Error ISBN Recrawler")
    print("="*60)
    print(f"üìÖ Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"‚öôÔ∏è  Batch size: {BATCH_SIZE} ISBNs per batch")
    print(f"‚öôÔ∏è  Word batch size: {WORD_BATCH_SIZE} words per batch")
    print()

    # 1. ISBN Î™©Î°ù ÏùΩÍ∏∞
    print("üìñ Reading isbn_with_error_message_only.txt...")
    isbns = read_isbn_list('isbn_with_error_message_only.txt')

    if not isbns:
        print("‚ùå No ISBNs found in file!")
        return

    stats['total_books'] = len(isbns)
    print(f"‚úÖ Found {len(isbns)} ISBNs to recrawl")
    print()

    # 2. ISBNÏùÑ Î∞∞ÏπòÎ°ú ÎÇòÎàÑÍ∏∞
    batches = [isbns[i:i + BATCH_SIZE] for i in range(0, len(isbns), BATCH_SIZE)]
    total_batches = len(batches)

    print(f"üì¶ Split into {total_batches} batches\n")

    # 3. Í∞Å Î∞∞Ïπò Ï≤òÎ¶¨
    start_time = datetime.now()

    for batch_num, batch in enumerate(batches, 1):
        try:
            await process_isbn_batch(batch, batch_num, total_batches)
        except Exception as e:
            print(f"‚ùå Error processing batch {batch_num}: {e}")
            stats['errors'] += 1
            continue

    # 4. ÏµúÏ¢Ö ÌÜµÍ≥Ñ
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print("\n" + "="*60)
    print("‚úÖ RECRAWLING COMPLETED")
    print("="*60)
    print(f"üìÖ End time: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"‚è±Ô∏è  Duration: {duration:.2f} seconds ({duration/60:.2f} minutes)")
    print()
    print("üìä Final Statistics:")
    print(f"   - Total books: {stats['total_books']}")
    print(f"   - Books processed: {stats['books_processed']}")
    print(f"   - Books with words: {stats['books_with_words']}")
    print(f"   - Books still error: {stats['books_still_error']}")
    print(f"   - Success rate: {stats['books_with_words']/stats['total_books']*100:.1f}%")
    print(f"   - Total words in lists: {stats['total_words_in_lists']}")
    print(f"   - Unique words saved: {stats['unique_words_saved']}")
    print(f"   - Words already existed: {stats['words_already_existed']}")
    print(f"   - Total unique words: {stats['unique_words_saved'] + stats['words_already_existed']}")
    print(f"   - Errors: {stats['errors']}")
    print()
    if duration > 0:
        print(f"‚ö° Average speed: {stats['books_processed']/duration:.2f} books/second")
    print("="*60)


if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Interrupted by user")
        print(f"üìä Partial Statistics:")
        print(f"   - Books processed: {stats['books_processed']}/{stats['total_books']}")
        print(f"   - Books with words: {stats['books_with_words']}")
        print(f"   - Books still error: {stats['books_still_error']}")
        print(f"   - Unique words saved: {stats['unique_words_saved']}")
        sys.exit(1)
    except Exception as e:
        print(f"\n‚ùå Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
